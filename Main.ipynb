{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Datasets\n",
    "tr = pd.read_csv('train.csv')\n",
    "ts = pd.read_csv('test.csv')\n",
    "view = pd.read_csv('view_log.csv')\n",
    "item = pd.read_csv('item_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 237609 entries, 0 to 237608\n",
      "Data columns (total 7 columns):\n",
      "impression_id      237609 non-null object\n",
      "impression_time    237609 non-null datetime64[ns]\n",
      "user_id            237609 non-null int64\n",
      "app_code           237609 non-null int64\n",
      "os_version         237609 non-null object\n",
      "is_4G              237609 non-null int64\n",
      "is_click           237609 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 12.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Converting Impression_time field to datetime format in train and test dataset\n",
    "tr['impression_time'] = pd.to_datetime(tr.impression_time)\n",
    "ts['impression_time'] = pd.to_datetime(ts.impression_time)\n",
    "tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Training dataset contains 237609 records and no missing values</font>\n",
    "### <font color='green'>columns in train dataset</font>\n",
    "> impression_id - Unique id of the ad\n",
    "\n",
    "> impression_time - timestamp of the ad (Date Ranges between 2018-11-15 to 2018-12-13)\n",
    "\n",
    "> user_id - Unique Id of each user (74723 unique users)\n",
    "\n",
    "> app_code - Application Code for a partner website (490 unique partner websites)\n",
    "\n",
    "> os_version - Version of operating system. Three categories (latest-54.3%, intermediate-23.4%, old-22.2%)\n",
    "\n",
    "> is_4G - 1-Using 4G, 0-No 4G (0-63.9%, 1-36.1%)\n",
    "\n",
    "> is_click - target variable, 1-ad got clicked, 0-ad was not clicked (only 4.57% hit ratio)\n",
    "\n",
    "### <font color='green'>Test dataset contains 90675 records and no missing values</font>\n",
    "### <font color='green'>columns in test dataset</font>\n",
    "> impression_id - Unique id of the ad\n",
    "\n",
    "> impression_time - timestamp of the ad (Date Ranges between 2018-12-12 to 2018-12-18)\n",
    "\n",
    "> user_id - Unique Id of each user (34079 unique users)\n",
    "\n",
    "> app_code - Application Code for a partner website (373 unique partner websites)\n",
    "\n",
    "> os_version - Version of operating system. Three categories (latest-53.8%, intermediate-23.4%, old-22.7%)\n",
    "\n",
    "> is_4G - 1-Using 4G, 0-No 4G (0-64.2%, 1-35.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132761 entries, 0 to 132760\n",
      "Data columns (total 6 columns):\n",
      "item_id         132761 non-null int64\n",
      "item_price      132761 non-null int64\n",
      "category_1      132761 non-null int64\n",
      "category_2      132761 non-null int64\n",
      "category_3      132761 non-null int64\n",
      "product_type    132761 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "#Analysing Item dataset\n",
    "item.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Item dataset contains 132761 records and no missing values</font>\n",
    "### <font color='green'>columns in item dataset</font>\n",
    "> item_id - Unique id of the item\n",
    "\n",
    "> item_price - price of the item (min:5, max=1340800, median=2944, mean=10826), Right skewness in price\n",
    "\n",
    "> category_1 - category depth 1 (17 unique category_1)\n",
    "\n",
    "> category_2 - category depth 2 (79 unique category_2)\n",
    "\n",
    "> category_3 - category depth 2 (335 unique category_3)\n",
    "\n",
    "> product_type - anonymized item type (7959 item_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3118622 entries, 0 to 3118621\n",
      "Data columns (total 5 columns):\n",
      "server_time    datetime64[ns]\n",
      "device_type    object\n",
      "session_id     int64\n",
      "user_id        int64\n",
      "item_id        int64\n",
      "dtypes: datetime64[ns](1), int64(3), object(1)\n",
      "memory usage: 119.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#Converting Impression_time field to datetime format in train and test dataset\n",
    "view['server_time'] = pd.to_datetime(view.server_time)\n",
    "#Anlaysing view_log dataset\n",
    "view.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>View_log dataset contains 3118622 records and no missing values</font>\n",
    "### <font color='green'>columns in View_log dataset</font>\n",
    "> server_time - Timestamp of the log (Date Ranges between 2018-10-15 to 2018-12-11)\n",
    "\n",
    "> device_type - Device type of the user. Three categories(android-99.97%, 0.027% from iphone, less than 0.001% from web)\n",
    "\n",
    "> session_id - Browser session id (1014970 unique sessions)\n",
    "\n",
    "> user_id - user id (89157 unique user id's)\n",
    "\n",
    "> item_id - item id (126708 unique items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing user level aggregation data and impression level aggregation data\n",
    "user_agg = pd.read_csv('user_agg.csv')\n",
    "imp_agg = pd.read_csv('imp_agg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deriving dayoftheweek, month, hour and day from impression_time in train and test dataset\n",
    "tr['impression_weekday_name'] = tr.impression_time.dt.weekday_name\n",
    "tr['impression_month'] = tr.impression_time.dt.month\n",
    "tr['impression_hour'] = tr.impression_time.dt.round('H').dt.hour\n",
    "tr['impression_day'] = tr.impression_time.dt.day\n",
    "\n",
    "ts['impression_weekday_name'] = ts.impression_time.dt.weekday_name\n",
    "ts['impression_month'] = ts.impression_time.dt.month\n",
    "ts['impression_hour'] = ts.impression_time.dt.round('H').dt.hour\n",
    "ts['impression_day'] = ts.impression_time.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Encoding on training data\n",
    "def MeanEncoding(df, col, trgt, alpha=5, splits=4):\n",
    "    mean_g = tr[trgt].mean()\n",
    "    newcol = col+'_Enc'\n",
    "    df[newcol] = np.nan\n",
    "    kf = KFold(n_splits=splits, random_state=100, shuffle=True)\n",
    "    for tr_idx, ts_idx in kf.split(tr):\n",
    "        enc_tr = df.loc[tr_idx]\n",
    "        enc_ts = df.loc[ts_idx]\n",
    "        map_enc = enc_tr.groupby([col])[trgt].describe().apply(lambda x: ((x['count']*x['mean'])+(mean_g*alpha))/\\\n",
    "                                                               (x['count']+alpha), axis=1)\n",
    "        df.loc[ts_idx, newcol] = enc_ts[col].map(map_enc)\n",
    "        \n",
    "    df[newcol] = df[newcol].astype('float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Encoding with 4folds and regularisation parameter(alpha) as 5\n",
    "col_list =['user_id', 'app_code', 'os_version', 'is_4G', 'impression_weekday_name', 'impression_hour',\n",
    "           'impression_day', 'impression_month']\n",
    "\n",
    "trgt = 'is_click'\n",
    "\n",
    "for x in col_list:\n",
    "    newcol = x+'_Enc'\n",
    "    ts[newcol] = np.nan\n",
    "    tr = MeanEncoding(tr, col=x, trgt=trgt)    \n",
    "    map_enc = tr.groupby([x])[newcol].mean()\n",
    "    ts[newcol] = ts[x].map(map_enc)\n",
    "    ts[newcol] = ts[newcol].astype('float')\n",
    "    tr.loc[tr[newcol].isnull(), newcol] = tr[trgt].mean()\n",
    "    ts.loc[ts[newcol].isnull(), newcol] = tr[trgt].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing model data\n",
    "#columns to drop\n",
    "drop_col = ['impression_time', 'app_code', 'os_version', 'is_4G', 'impression_weekday_name', 'impression_hour',\n",
    "           'impression_day', 'impression_month']\n",
    "    \n",
    "tr1 = tr.drop(drop_col, axis='columns')\n",
    "#Merging user aggregation data to train dataset based on user_id\n",
    "tr1 = tr1.merge(user_agg, on='user_id', how='left').drop(['user_id'], axis='columns')\n",
    "\n",
    "#Merging user aggregation data to train dataset based on impression_id\n",
    "model_data = tr1.merge(imp_agg, on='impression_id', how='left').drop(['impression_id'], axis='columns')\n",
    "\n",
    "for x1 in user_agg.drop(['user_id'], axis='columns').columns:\n",
    "    model_data.loc[model_data[x1].isnull(), x1] = 0\n",
    "    \n",
    "for x1 in imp_agg.drop(['impression_id'], axis='columns').columns:\n",
    "    model_data.loc[model_data[x1].isnull(), x1] = 0\n",
    "\n",
    "\n",
    "#preparing training and validation datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_data.drop(['is_click'], axis=1),\\\n",
    "                                                    model_data['is_click'],\\\n",
    "                                                    test_size=0.25, random_state=100, stratify=model_data['is_click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "app_code_Enc                               0.211349\n",
      "user_id_Enc                                0.147234\n",
      "user_session_id_nunique                    0.033343\n",
      "user_category_3_nunique                    0.021822\n",
      "user_server_month_12_item_count            0.020892\n",
      "user_server_month_12_item_nunique          0.019327\n",
      "user_item_id_nunique                       0.018367\n",
      "user_server_weekday_Sunday_item_nunique    0.017153\n",
      "user_category_2_nunique                    0.016924\n",
      "imp_server_month_12_item_nunique           0.015522\n",
      "dtype: float64\n",
      "0.7099464577124308\n",
      "[[107370  62690]\n",
      " [  2656   5490]]\n",
      "0.6974393410286228\n",
      "[[35765 20922]\n",
      " [  894  1822]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=100, n_jobs=-1,\n",
    "                            oob_score=True, class_weight=\"balanced_subsample\")\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Feature Importance:\\n\"+\n",
    "      str(pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False).head(10)))\n",
    "\n",
    "print(roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print(confusion_matrix(y_train, rf.predict(X_train)))\n",
    "print(roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
    "print(confusion_matrix(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.074288\n",
      "0:\ttest: 0.6230736\tbest: 0.6230736 (0)\ttotal: 835ms\tremaining: 55m 38s\n",
      "500:\ttest: 0.7526571\tbest: 0.7527189 (487)\ttotal: 3m 28s\tremaining: 24m 14s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7527189098\n",
      "bestIteration = 487\n",
      "\n",
      "Shrink model to first 488 iterations.\n",
      "0.7857968100194501\n",
      "0.7527189098195806\n"
     ]
    }
   ],
   "source": [
    "#Testing Catboost Model\n",
    "cat = CatBoostClassifier(iterations=4000, eval_metric='AUC')\n",
    "fit_params = {'early_stopping_rounds': 100, 'eval_set': [(X_test, y_test)], 'verbose': 500}\n",
    "cat.fit(X_train, y_train, **fit_params)\n",
    "y_pred1_prob = cat.predict_proba(X_train)[:,1]\n",
    "y_pred2_prob = cat.predict_proba(X_test)[:,1]\n",
    "print(roc_auc_score(y_train, y_pred1_prob))\n",
    "print(roc_auc_score(y_test, y_pred2_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.803761\ttraining's binary_logloss: 0.157459\tvalid_1's auc: 0.75948\tvalid_1's binary_logloss: 0.165682\n",
      "Early stopping, best iteration is:\n",
      "[439]\ttraining's auc: 0.799019\ttraining's binary_logloss: 0.158448\tvalid_1's auc: 0.759628\tvalid_1's binary_logloss: 0.165679\n",
      "Feature Importance:\n",
      "user_id_Enc                          207\n",
      "app_code_Enc                         206\n",
      "imp_server_month_12_item_count        60\n",
      "user_session_id_nunique               52\n",
      "impression_hour_Enc                   49\n",
      "impression_day_Enc                    42\n",
      "impression_weekday_name_Enc           41\n",
      "impression_month_Enc                  40\n",
      "user_server_month_12_item_nunique     39\n",
      "os_version_Enc                        28\n",
      "dtype: int32\n",
      "0.7990188227063546\n",
      "0.7596281130398165\n"
     ]
    }
   ],
   "source": [
    "#Testing LGBM Model\n",
    "lgbm = LGBMClassifier(learning_rate=0.05, colsample_bytree=0.5, subsample=0.8, subsample_freq=1,\\\n",
    "                      max_bin=31, n_estimators=4000, min_child_samples= 250, num_leaves=8,\\\n",
    "                      objective='binary',scale_pos_weight=1)\n",
    "fit_params = {'early_stopping_rounds': 100, 'eval_set': [(X_train, y_train),\n",
    "                                                         (X_test, y_test)],\n",
    "              'verbose': 500, 'eval_metric': 'auc'}\n",
    "\n",
    "lgbm.fit(X_train, y_train, **fit_params)\n",
    "print(\"Feature Importance:\\n\"+\n",
    "      str(pd.Series(lgbm.feature_importances_, index=X_train.columns).sort_values(ascending=False).head(10)))\n",
    "y_pred1_prob = lgbm.predict_proba(X_train)[:,1]\n",
    "y_pred2_prob = lgbm.predict_proba(X_test)[:,1]\n",
    "print(roc_auc_score(y_train, y_pred1_prob))\n",
    "print(roc_auc_score(y_test, y_pred2_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected LGBM model based on accuracy and speed\n",
    "#Preparing train and test data  for final output\n",
    "#Preparing model data\n",
    "#columns to drop\n",
    "drop_col = ['impression_time', 'app_code', 'os_version', 'is_4G', 'impression_weekday_name', 'impression_hour',\n",
    "           'impression_day', 'impression_month']\n",
    "    \n",
    "tr1 = tr.drop(drop_col, axis='columns')\n",
    "#Merging user aggregation data to train dataset based on user_id\n",
    "tr1 = tr1.merge(user_agg, on='user_id', how='left').drop(['user_id'], axis='columns')\n",
    "\n",
    "#Merging user aggregation data to train dataset based on impression_id\n",
    "model_data = tr1.merge(imp_agg, on='impression_id', how='left').drop(['impression_id'], axis='columns')\n",
    "\n",
    "for x1 in user_agg.drop(['user_id'], axis='columns').columns:\n",
    "    model_data.loc[model_data[x1].isnull(), x1] = 0\n",
    "    \n",
    "for x1 in imp_agg.drop(['impression_id'], axis='columns').columns:\n",
    "    model_data.loc[model_data[x1].isnull(), x1] = 0\n",
    "\n",
    "\n",
    "#preparing training and validation datasets\n",
    "X_train, y_train = model_data.drop(['is_click'], axis=1), model_data['is_click']\n",
    "\n",
    "#Preparing test_data\n",
    "ts1 = ts.drop(drop_col, axis='columns')\n",
    "#Merging user aggregation data to train dataset based on user_id\n",
    "ts1 = ts1.merge(user_agg, on='user_id', how='left').drop(['user_id'], axis='columns')\n",
    "\n",
    "#Merging user aggregation data to train dataset based on impression_id\n",
    "test_data = ts1.merge(imp_agg, on='impression_id', how='left').drop(['impression_id'], axis='columns')\n",
    "\n",
    "for x1 in user_agg.drop(['user_id'], axis='columns').columns:\n",
    "    test_data.loc[test_data[x1].isnull(), x1] = 0\n",
    "    \n",
    "for x1 in imp_agg.drop(['impression_id'], axis='columns').columns:\n",
    "    test_data.loc[test_data[x1].isnull(), x1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldvalidationLGBM(X_train, y_train, X_test, splits=10):\n",
    "    skf = StratifiedKFold(n_splits=splits, random_state=100, shuffle=True)\n",
    "    y_pred_tot=[]\n",
    "    y_tmp_ts1=[]\n",
    "    y_tmp_pred=[]\n",
    "    \n",
    "    for i, idx in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr1, y_tr1 = X_train.iloc[idx[0]], y_train.iloc[idx[0]]\n",
    "        X_ts1, y_ts1 = X_train.iloc[idx[1]], y_train.iloc[idx[1]]\n",
    "        \n",
    "        lgbm = LGBMClassifier(learning_rate=0.05, colsample_bytree=0.5, subsample=0.8, subsample_freq=1,\\\n",
    "                      max_bin=31, n_estimators=4000, min_child_samples= 250, num_leaves=8,\\\n",
    "                      objective='binary',scale_pos_weight=1)\n",
    "        #reg_alpha=0.1, reg_lambda=0.1\n",
    "        fit_params = {'early_stopping_rounds': 100, 'eval_set': [(X_tr1, y_tr1), (X_ts1, y_ts1)],\n",
    "                      'verbose': 500, 'eval_metric': 'auc'}\n",
    "        lgbm.fit(X_tr1, y_tr1, **fit_params)\n",
    "        print('Fold :',i+1)\n",
    "        pred_ts1 = lgbm.predict_proba(X_ts1, num_iteration=lgbm.best_iteration_)[:, 1]\n",
    "        print('AUC Score:\\t',roc_auc_score(y_ts1, pred_ts1))\n",
    "        y_tmp_ts1 =np.concatenate((y_tmp_ts1, y_ts1))\n",
    "        y_tmp_pred =np.concatenate((y_tmp_pred, pred_ts1))\n",
    "        pred_test = lgbm.predict_proba(X_test)[:,1]\n",
    "        y_pred_tot.append(pred_test)\n",
    "    \n",
    "    print('Total AUC Score:\\t', roc_auc_score(y_tmp_ts1, y_tmp_pred))\n",
    "    return np.mean(y_pred_tot, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.797486\ttraining's binary_logloss: 0.158598\tvalid_1's auc: 0.765982\tvalid_1's binary_logloss: 0.165097\n",
      "Early stopping, best iteration is:\n",
      "[675]\ttraining's auc: 0.807137\ttraining's binary_logloss: 0.156463\tvalid_1's auc: 0.767018\tvalid_1's binary_logloss: 0.16495\n",
      "Fold : 1\n",
      "AUC Score:\t 0.767017889074955\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.798491\ttraining's binary_logloss: 0.158486\tvalid_1's auc: 0.762968\tvalid_1's binary_logloss: 0.165043\n",
      "Early stopping, best iteration is:\n",
      "[481]\ttraining's auc: 0.797401\ttraining's binary_logloss: 0.158707\tvalid_1's auc: 0.763357\tvalid_1's binary_logloss: 0.164994\n",
      "Fold : 2\n",
      "AUC Score:\t 0.763356760106663\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.798561\ttraining's binary_logloss: 0.15844\tvalid_1's auc: 0.757447\tvalid_1's binary_logloss: 0.166111\n",
      "Early stopping, best iteration is:\n",
      "[659]\ttraining's auc: 0.807777\ttraining's binary_logloss: 0.156434\tvalid_1's auc: 0.757923\tvalid_1's binary_logloss: 0.166077\n",
      "Fold : 3\n",
      "AUC Score:\t 0.7579225016801996\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[329]\ttraining's auc: 0.787116\ttraining's binary_logloss: 0.16076\tvalid_1's auc: 0.757219\tvalid_1's binary_logloss: 0.166056\n",
      "Fold : 4\n",
      "AUC Score:\t 0.7572194980314761\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.797127\ttraining's binary_logloss: 0.158648\tvalid_1's auc: 0.775203\tvalid_1's binary_logloss: 0.163649\n",
      "Early stopping, best iteration is:\n",
      "[558]\ttraining's auc: 0.800624\ttraining's binary_logloss: 0.157844\tvalid_1's auc: 0.775509\tvalid_1's binary_logloss: 0.163588\n",
      "Fold : 5\n",
      "AUC Score:\t 0.7755090852607405\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.798569\ttraining's binary_logloss: 0.15851\tvalid_1's auc: 0.76464\tvalid_1's binary_logloss: 0.165271\n",
      "[1000]\ttraining's auc: 0.823767\ttraining's binary_logloss: 0.152624\tvalid_1's auc: 0.76756\tvalid_1's binary_logloss: 0.164609\n",
      "Early stopping, best iteration is:\n",
      "[1002]\ttraining's auc: 0.823871\ttraining's binary_logloss: 0.152605\tvalid_1's auc: 0.767652\tvalid_1's binary_logloss: 0.164592\n",
      "Fold : 6\n",
      "AUC Score:\t 0.7676521875082488\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.798232\ttraining's binary_logloss: 0.158546\tvalid_1's auc: 0.764015\tvalid_1's binary_logloss: 0.16476\n",
      "[1000]\ttraining's auc: 0.822814\ttraining's binary_logloss: 0.152838\tvalid_1's auc: 0.765097\tvalid_1's binary_logloss: 0.16462\n",
      "Early stopping, best iteration is:\n",
      "[916]\ttraining's auc: 0.819739\ttraining's binary_logloss: 0.153678\tvalid_1's auc: 0.765544\tvalid_1's binary_logloss: 0.164567\n",
      "Fold : 7\n",
      "AUC Score:\t 0.7655441714839157\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.800502\ttraining's binary_logloss: 0.158037\tvalid_1's auc: 0.751913\tvalid_1's binary_logloss: 0.167098\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's auc: 0.794245\ttraining's binary_logloss: 0.15935\tvalid_1's auc: 0.752421\tvalid_1's binary_logloss: 0.167032\n",
      "Fold : 8\n",
      "AUC Score:\t 0.7524209546440206\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.798472\ttraining's binary_logloss: 0.15849\tvalid_1's auc: 0.765396\tvalid_1's binary_logloss: 0.165111\n",
      "Early stopping, best iteration is:\n",
      "[518]\ttraining's auc: 0.799831\ttraining's binary_logloss: 0.158224\tvalid_1's auc: 0.765785\tvalid_1's binary_logloss: 0.165071\n",
      "Fold : 9\n",
      "AUC Score:\t 0.765784765604758\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.797904\ttraining's binary_logloss: 0.158621\tvalid_1's auc: 0.766934\tvalid_1's binary_logloss: 0.164123\n",
      "Early stopping, best iteration is:\n",
      "[514]\ttraining's auc: 0.798657\ttraining's binary_logloss: 0.158461\tvalid_1's auc: 0.76698\tvalid_1's binary_logloss: 0.164087\n",
      "Fold : 10\n",
      "AUC Score:\t 0.7669802067611858\n",
      "Total AUC Score:\t 0.7638900903618493\n"
     ]
    }
   ],
   "source": [
    "#Calculating final prediction based on averaged 10fold predictions\n",
    "pred_lgbm = kfoldvalidationLGBM(X_train, y_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prparing final probability data for submission\n",
    "out = pd.DataFrame({'impression_id': ts['impression_id'], 'is_click': pred_lgbm})\n",
    "out.to_csv('KFoldLGBM_sub1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
